{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "17dbb5aa",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2fdb7e7c6fb4b57aa7e649fb61cddc6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FileUpload(value={}, accept='.py', description='Upload diffusion/*.py', multiple=True)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "BASE_DIR = \"/content\"\n",
        "DIFF_DIR = os.path.join(BASE_DIR, \"diffusion\")\n",
        "os.makedirs(DIFF_DIR, exist_ok=True)\n",
        "\n",
        "uploader = widgets.FileUpload(\n",
        "    accept=\".py\",\n",
        "    multiple=True,\n",
        "    description=\"Upload diffusion/*.py\",\n",
        ")\n",
        "\n",
        "def handle_upload(change):\n",
        "    os.makedirs(DIFF_DIR, exist_ok=True)\n",
        "    for name, meta in uploader.value.items():\n",
        "        data = meta[\"content\"]\n",
        "        path = os.path.join(DIFF_DIR, os.path.basename(name))\n",
        "        with open(path, \"wb\") as f:\n",
        "            f.write(data)\n",
        "    print(f\"Saved {len(uploader.value)} files to {DIFF_DIR}\")\n",
        "\n",
        "uploader.observe(handle_upload, names=\"value\")\n",
        "display(uploader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100% 9.91M/9.91M [00:02<00:00, 4.90MB/s]\n",
            "100% 28.9k/28.9k [00:00<00:00, 133kB/s]\n",
            "100% 1.65M/1.65M [00:01<00:00, 1.25MB/s]\n",
            "100% 4.54k/4.54k [00:00<00:00, 16.4MB/s]\n",
            "Training...\n",
            "step 10, loss: 0.042696359821341255\n",
            "step 20, loss: 0.04249207764154389\n",
            "step 30, loss: 0.04146890486440351\n",
            "step 40, loss: 0.04109067851450385\n",
            "step 50, loss: 0.040139799718471134\n",
            "step 60, loss: 0.04040001231993808\n",
            "step 70, loss: 0.04073774455193902\n",
            "step 80, loss: 0.04023328756936538\n",
            "step 90, loss: 0.03949788225057361\n",
            "step 100, loss: 0.039093541745096445\n",
            "step 110, loss: 0.03785747719928622\n",
            "step 120, loss: 0.036597183961421254\n",
            "step 130, loss: 0.035693791974335906\n",
            "step 140, loss: 0.034759864546358585\n",
            "step 150, loss: 0.033985293470323086\n",
            "step 160, loss: 0.03239469362422824\n",
            "step 170, loss: 0.031173083037137984\n",
            "step 180, loss: 0.030190857760608196\n",
            "step 190, loss: 0.029333246257156134\n",
            "step 200, loss: 0.028749567419290543\n",
            "step 210, loss: 0.028197565134614706\n",
            "step 220, loss: 0.027710045389831066\n",
            "step 230, loss: 0.027015726640820505\n",
            "step 240, loss: 0.02642991131171584\n",
            "step 250, loss: 0.026261075250804424\n",
            "step 260, loss: 0.02609904369339347\n",
            "step 270, loss: 0.025741601698100566\n",
            "step 280, loss: 0.025312009528279306\n",
            "step 290, loss: 0.025208301935344936\n",
            "step 300, loss: 0.024800188038498164\n",
            "step 310, loss: 0.024542062561959028\n",
            "step 320, loss: 0.02495456898584962\n",
            "step 330, loss: 0.02512378478422761\n",
            "step 340, loss: 0.02568764701485634\n",
            "step 350, loss: 0.025233059544116258\n",
            "step 360, loss: 0.02516935784369707\n",
            "step 370, loss: 0.02481654169037938\n",
            "step 380, loss: 0.024907730650156737\n",
            "step 390, loss: 0.024568723421543835\n",
            "step 400, loss: 0.024952185954898596\n",
            "step 410, loss: 0.02457425158470869\n",
            "step 420, loss: 0.023937982711941003\n",
            "step 430, loss: 0.02344054833985865\n",
            "step 440, loss: 0.022484131837263704\n",
            "step 450, loss: 0.02243233094923198\n",
            "step 460, loss: 0.022381733665242792\n",
            "step 470, loss: 0.02209570470266044\n",
            "step 480, loss: 0.02201950444839895\n",
            "step 490, loss: 0.02194551910273731\n",
            "step 500, loss: 0.021227282667532564\n",
            "step 510, loss: 0.02115871334448457\n",
            "step 520, loss: 0.021026778146624566\n",
            "step 530, loss: 0.021512079564854503\n",
            "step 540, loss: 0.02151151024736464\n",
            "step 550, loss: 0.02110096243210137\n",
            "step 560, loss: 0.02106072025373578\n",
            "step 570, loss: 0.021116281719878317\n",
            "step 580, loss: 0.020781521750614047\n",
            "step 590, loss: 0.020844937423244118\n",
            "step 600, loss: 0.020489841429516674\n",
            "step 610, loss: 0.020521693248301744\n",
            "step 620, loss: 0.02023124380968511\n",
            "step 630, loss: 0.019603186175227164\n",
            "step 640, loss: 0.019141010139137507\n",
            "step 650, loss: 0.019093340216204523\n",
            "step 660, loss: 0.018785470016300678\n",
            "step 670, loss: 0.018538521695882083\n",
            "step 680, loss: 0.018438632218167186\n",
            "step 690, loss: 0.01807271090336144\n",
            "step 700, loss: 0.01820722982287407\n",
            "step 710, loss: 0.018177828872576355\n",
            "step 720, loss: 0.01776572766713798\n",
            "step 730, loss: 0.01748018553480506\n",
            "step 740, loss: 0.017660634564235805\n",
            "step 750, loss: 0.017645745100453497\n",
            "step 760, loss: 0.017194253765046595\n",
            "step 770, loss: 0.016925556445494296\n",
            "step 780, loss: 0.01644174468703568\n",
            "step 790, loss: 0.01609320702031255\n",
            "step 800, loss: 0.015899662636220457\n",
            "step 810, loss: 0.015524603929370641\n",
            "step 820, loss: 0.015372024411335587\n",
            "step 830, loss: 0.015408799657598138\n",
            "step 840, loss: 0.015295693036168814\n",
            "step 850, loss: 0.015418417835608125\n",
            "step 860, loss: 0.015711032301187516\n",
            "step 870, loss: 0.01575928089208901\n",
            "step 880, loss: 0.015795765984803437\n",
            "step 890, loss: 0.01580944704823196\n",
            "step 900, loss: 0.015448549771681428\n",
            "step 910, loss: 0.015424214852973819\n",
            "step 920, loss: 0.015561021585017442\n",
            "step 930, loss: 0.015423468416556715\n",
            "step 940, loss: 0.015320294136181474\n",
            "step 950, loss: 0.014918699963018298\n",
            "step 960, loss: 0.014643005756661297\n",
            "step 970, loss: 0.014481153842061757\n",
            "step 980, loss: 0.01445676626637578\n",
            "step 990, loss: 0.014416929129511117\n",
            "step 1000, loss: 0.014242428354918957\n",
            "step 1010, loss: 0.013967036586254835\n",
            "step 1020, loss: 0.013907699063420296\n",
            "step 1030, loss: 0.013752309717237949\n",
            "step 1040, loss: 0.013486538548022509\n",
            "step 1050, loss: 0.013531974665820599\n",
            "step 1060, loss: 0.013548153890296817\n",
            "step 1070, loss: 0.013334882846102118\n",
            "step 1080, loss: 0.01341435756534338\n",
            "step 1090, loss: 0.013439434310421348\n",
            "step 1100, loss: 0.013491120534017681\n",
            "step 1110, loss: 0.01344634521752596\n",
            "step 1120, loss: 0.013267196733504533\n",
            "step 1130, loss: 0.013076850939542055\n",
            "step 1140, loss: 0.013163834288716316\n",
            "step 1150, loss: 0.012960523329675198\n",
            "step 1160, loss: 0.012613485464826227\n",
            "step 1170, loss: 0.012431431142613292\n",
            "step 1180, loss: 0.012236199202015996\n",
            "step 1190, loss: 0.012127787852659822\n",
            "step 1200, loss: 0.011903354628011585\n",
            "step 1210, loss: 0.0118158876337111\n",
            "step 1220, loss: 0.011691748490557074\n",
            "step 1230, loss: 0.0116100930608809\n",
            "step 1240, loss: 0.01118167283013463\n",
            "step 1250, loss: 0.010902715558186173\n",
            "step 1260, loss: 0.010784884984605014\n",
            "step 1270, loss: 0.01092827417422086\n",
            "step 1280, loss: 0.010846640481613577\n",
            "step 1290, loss: 0.010791128105483948\n",
            "step 1300, loss: 0.010945258946157992\n",
            "step 1310, loss: 0.011145771006122232\n",
            "step 1320, loss: 0.011254671239294111\n",
            "step 1330, loss: 0.011225429787300527\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/diffusion/annealed_sm_mnist.py\", line 50, in <module>\n",
            "    model = train_annealed_mnist_score_matching(training_cfg)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/diffusion/annealed_sm_mnist.py\", line 27, in train_annealed_mnist_score_matching\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\", line 625, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\", line 354, in backward\n",
            "    _engine_run_backward(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\", line 841, in _engine_run_backward\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!uv run python -m diffusion.annealed_sm_mnist"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
